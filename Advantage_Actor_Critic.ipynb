{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices: []\n",
      "Warning! GPU memory could not be limitated!\n"
     ]
    }
   ],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Physical Devices: {}'.format(physical_devices))\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('GPU memory limitated successfuly!')\n",
    "except:\n",
    "    print('Warning! GPU memory could not be limitated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero poner el ambiente, mostrar el baseline, explicarlo y luego el algoritmo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an Environment and an Agent?<br>\n",
    "Take a look into the lunar Lander Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x7f2a3c01af70>,\n",
       " 'action_space': Discrete(4),\n",
       " 'observation_space': Box(-inf, inf, (8,), float32),\n",
       " 'reward_range': (-inf, inf),\n",
       " 'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "  'video.frames_per_second': 50},\n",
       " '_max_episode_steps': 1000,\n",
       " '_elapsed_steps': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from libs.Utils import plot_history\n",
    "from libs.Utils import test_agent\n",
    "\n",
    "# Load environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "# Number of actions\n",
    "n_actions = env.action_space.n\n",
    "vars(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the LunarLander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  284.2599077715691\n",
      "Total Episode Reward:  290.69490397375694\n",
      "Total Episode Reward:  265.8646263073205\n",
      "Total Episode Reward:  240.74516870263656\n",
      "Total Episode Reward:  293.8572048127621\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, heuristic=True, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma=0.99, actor_lr=5e-6, critic_lr=5e-6):\n",
    "        self.gamma = gamma\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        probs = self.actor(np.array([state]))\n",
    "        # tfp.distributions returns a prob dist\n",
    "        # same as using np.random.choice([0,1,2,3], p=probs.numpy())\n",
    "        dist = tfp.distributions.Categorical(probs=probs.numpy(), dtype=tf.float32)\n",
    "        action = dist.sample().numpy()\n",
    "        \n",
    "        return int(action[0])\n",
    "    \n",
    "    # Note this is actually the performance measure J(theta)\n",
    "    def actor_loss(self, probs, action, advantage):\n",
    "        dist = tfp.distributions.Categorical(probs=probs, dtype=tf.float32)\n",
    "        log_probs = dist.log_prob(action)\n",
    "        # Since we are maximizing the agent's performance,  we need to add a minus -\n",
    "        # to actually maximize instead of minimize\n",
    "        loss = -log_probs * advantage\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # tf needs a bidimensional array as input:\n",
    "        state = np.array([state])\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # Set costum losses for the actor and the critic\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            action_probs = self.actor(state, training=True)\n",
    "            state_value = self.critic(state, training=True)\n",
    "            if done:\n",
    "                next_state_value = 0\n",
    "            else:\n",
    "                next_state_value = self.critic(next_state, training=True)\n",
    "            \n",
    "            advantage = reward + self.gamma * next_state_value - state_value\n",
    "            \n",
    "            # Agents Performance measure J(theta)\n",
    "            agent_loss = self.actor_loss(action_probs, action, advantage)\n",
    "            # Critic loss (MSE for one example) is basically an \n",
    "            # approximation of (v - v_hat)^2\n",
    "            critic_loss = advantage**2\n",
    "        \n",
    "        # Apply learning rule\n",
    "        actor_grads = actor_tape.gradient(agent_loss, \n",
    "                                          self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(critic_loss, \n",
    "                                            self.critic.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, \n",
    "                                                 self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, \n",
    "                                                  self.critic.trainable_variables))\n",
    "        \n",
    "        return agent_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Factor\n",
    "gamma = 0.99\n",
    "# Actor learning rate\n",
    "actor_lr = 5e-6\n",
    "# Critic learning rate\n",
    "critic_lr = 5e-6\n",
    "\n",
    "# Init agent\n",
    "agent = Agent(gamma=0.99, actor_lr=5e-6, critic_lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our agent performs without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -114.73277826257134\n",
      "Total Episode Reward:  -221.0962736520064\n",
      "Total Episode Reward:  -305.23594311146314\n",
      "Total Episode Reward:  -81.8531241754936\n",
      "Total Episode Reward:  -153.59670515214492\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project'\n",
    "os.makedirs(os.path.join(path, 'Models'), exist_ok=True)\n",
    "model_path = os.path.join(path, 'Models', 'Advantage_Actor_Critic')\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode:  0\n",
      "episode  0 total episode reward -125.16, avg episode reward -125.16\n",
      "Starting episode:  1\n",
      "episode  1 total episode reward -92.40, avg episode reward -108.78\n",
      "Starting episode:  2\n",
      "episode  2 total episode reward -173.11, avg episode reward -130.22\n",
      "Starting episode:  3\n",
      "episode  3 total episode reward -102.62, avg episode reward -123.32\n",
      "Starting episode:  4\n",
      "episode  4 total episode reward -99.42, avg episode reward -118.54\n",
      "Starting episode:  5\n",
      "episode  5 total episode reward -439.67, avg episode reward -172.06\n",
      "Starting episode:  6\n",
      "episode  6 total episode reward -203.14, avg episode reward -176.50\n",
      "Starting episode:  7\n",
      "episode  7 total episode reward -290.72, avg episode reward -190.78\n",
      "Starting episode:  8\n",
      "episode  8 total episode reward -210.41, avg episode reward -192.96\n",
      "Starting episode:  9\n",
      "episode  9 total episode reward -300.16, avg episode reward -203.68\n",
      "Starting episode:  10\n",
      "episode  10 total episode reward -376.36, avg episode reward -219.38\n",
      "Starting episode:  11\n",
      "episode  11 total episode reward -126.43, avg episode reward -211.63\n",
      "Starting episode:  12\n",
      "episode  12 total episode reward -92.44, avg episode reward -202.47\n",
      "Starting episode:  13\n",
      "episode  13 total episode reward -407.72, avg episode reward -217.13\n",
      "Starting episode:  14\n",
      "episode  14 total episode reward -216.81, avg episode reward -217.11\n",
      "Starting episode:  15\n",
      "episode  15 total episode reward 25.28, avg episode reward -201.96\n",
      "Starting episode:  16\n",
      "episode  16 total episode reward -329.37, avg episode reward -209.45\n",
      "Starting episode:  17\n",
      "episode  17 total episode reward -473.96, avg episode reward -224.15\n",
      "Starting episode:  18\n",
      "episode  18 total episode reward -48.46, avg episode reward -214.90\n",
      "Starting episode:  19\n",
      "episode  19 total episode reward -81.26, avg episode reward -208.22\n",
      "Starting episode:  20\n",
      "episode  20 total episode reward -229.28, avg episode reward -209.22\n",
      "Starting episode:  21\n",
      "episode  21 total episode reward -277.53, avg episode reward -212.33\n",
      "Starting episode:  22\n",
      "episode  22 total episode reward -65.40, avg episode reward -205.94\n",
      "Starting episode:  23\n",
      "episode  23 total episode reward -171.94, avg episode reward -204.52\n",
      "Starting episode:  24\n",
      "episode  24 total episode reward -244.12, avg episode reward -206.11\n",
      "Starting episode:  25\n",
      "episode  25 total episode reward -318.12, avg episode reward -210.41\n",
      "Starting episode:  26\n",
      "episode  26 total episode reward -84.35, avg episode reward -205.74\n",
      "Starting episode:  27\n",
      "episode  27 total episode reward -301.15, avg episode reward -209.15\n",
      "Starting episode:  28\n",
      "episode  28 total episode reward -173.67, avg episode reward -207.93\n",
      "Starting episode:  29\n",
      "episode  29 total episode reward -247.79, avg episode reward -209.26\n",
      "Starting episode:  30\n",
      "episode  30 total episode reward -181.13, avg episode reward -208.35\n",
      "Starting episode:  31\n",
      "episode  31 total episode reward -158.89, avg episode reward -206.80\n",
      "Starting episode:  32\n",
      "episode  32 total episode reward -186.56, avg episode reward -206.19\n",
      "Starting episode:  33\n",
      "episode  33 total episode reward -45.56, avg episode reward -201.47\n",
      "Starting episode:  34\n",
      "episode  34 total episode reward -119.06, avg episode reward -199.11\n",
      "Starting episode:  35\n",
      "episode  35 total episode reward -49.87, avg episode reward -194.97\n",
      "Starting episode:  36\n",
      "episode  36 total episode reward -417.62, avg episode reward -200.98\n",
      "Starting episode:  37\n",
      "episode  37 total episode reward -443.81, avg episode reward -207.37\n",
      "Starting episode:  38\n",
      "episode  38 total episode reward -105.02, avg episode reward -204.75\n",
      "Starting episode:  39\n",
      "episode  39 total episode reward -318.46, avg episode reward -207.59\n",
      "Starting episode:  40\n",
      "episode  40 total episode reward -106.51, avg episode reward -205.13\n",
      "Starting episode:  41\n",
      "episode  41 total episode reward -214.99, avg episode reward -205.36\n",
      "Starting episode:  42\n",
      "episode  42 total episode reward -163.12, avg episode reward -204.38\n",
      "Starting episode:  43\n",
      "episode  43 total episode reward -218.06, avg episode reward -204.69\n",
      "Starting episode:  44\n",
      "episode  44 total episode reward -216.82, avg episode reward -204.96\n",
      "Starting episode:  45\n",
      "episode  45 total episode reward -226.24, avg episode reward -205.42\n",
      "Starting episode:  46\n",
      "episode  46 total episode reward -165.81, avg episode reward -204.58\n",
      "Starting episode:  47\n",
      "episode  47 total episode reward -151.64, avg episode reward -203.48\n",
      "Starting episode:  48\n",
      "episode  48 total episode reward -159.84, avg episode reward -202.59\n",
      "Starting episode:  49\n",
      "episode  49 total episode reward -282.89, avg episode reward -204.19\n",
      "Starting episode:  50\n",
      "episode  50 total episode reward -54.57, avg episode reward -201.26\n",
      "Starting episode:  51\n",
      "episode  51 total episode reward -187.49, avg episode reward -200.99\n",
      "Starting episode:  52\n",
      "episode  52 total episode reward -101.31, avg episode reward -199.11\n",
      "Starting episode:  53\n",
      "episode  53 total episode reward -269.48, avg episode reward -200.42\n",
      "Starting episode:  54\n",
      "episode  54 total episode reward -177.42, avg episode reward -200.00\n",
      "Starting episode:  55\n",
      "episode  55 total episode reward -158.72, avg episode reward -199.26\n",
      "Starting episode:  56\n",
      "episode  56 total episode reward -210.73, avg episode reward -199.46\n",
      "Starting episode:  57\n",
      "episode  57 total episode reward -266.11, avg episode reward -200.61\n",
      "Starting episode:  58\n",
      "episode  58 total episode reward -323.90, avg episode reward -202.70\n",
      "Starting episode:  59\n",
      "episode  59 total episode reward -104.70, avg episode reward -201.07\n",
      "Starting episode:  60\n",
      "episode  60 total episode reward -242.37, avg episode reward -201.74\n",
      "Starting episode:  61\n",
      "episode  61 total episode reward -325.19, avg episode reward -203.74\n",
      "Starting episode:  62\n",
      "episode  62 total episode reward -111.57, avg episode reward -202.27\n",
      "Starting episode:  63\n",
      "episode  63 total episode reward -138.75, avg episode reward -201.28\n",
      "Starting episode:  64\n",
      "episode  64 total episode reward -40.11, avg episode reward -198.80\n",
      "Starting episode:  65\n",
      "episode  65 total episode reward -223.91, avg episode reward -199.18\n",
      "Starting episode:  66\n",
      "episode  66 total episode reward -131.69, avg episode reward -198.17\n",
      "Starting episode:  67\n",
      "episode  67 total episode reward -348.07, avg episode reward -200.38\n",
      "Starting episode:  68\n",
      "episode  68 total episode reward -5.64, avg episode reward -197.56\n",
      "Starting episode:  69\n",
      "episode  69 total episode reward -103.01, avg episode reward -196.21\n",
      "Starting episode:  70\n",
      "episode  70 total episode reward -77.50, avg episode reward -194.53\n",
      "Starting episode:  71\n",
      "episode  71 total episode reward -153.48, avg episode reward -193.96\n",
      "Starting episode:  72\n",
      "episode  72 total episode reward -189.56, avg episode reward -193.90\n",
      "Starting episode:  73\n",
      "episode  73 total episode reward -280.75, avg episode reward -195.08\n",
      "Starting episode:  74\n",
      "episode  74 total episode reward -257.12, avg episode reward -195.90\n",
      "Starting episode:  75\n",
      "episode  75 total episode reward -115.20, avg episode reward -194.84\n",
      "Starting episode:  76\n",
      "episode  76 total episode reward -74.08, avg episode reward -193.27\n",
      "Starting episode:  77\n",
      "episode  77 total episode reward -111.23, avg episode reward -192.22\n",
      "Starting episode:  78\n",
      "episode  78 total episode reward -35.82, avg episode reward -190.24\n",
      "Starting episode:  79\n",
      "episode  79 total episode reward -186.73, avg episode reward -190.20\n",
      "Starting episode:  80\n",
      "episode  80 total episode reward -87.53, avg episode reward -188.93\n",
      "Starting episode:  81\n",
      "episode  81 total episode reward -142.09, avg episode reward -188.36\n",
      "Starting episode:  82\n",
      "episode  82 total episode reward -173.40, avg episode reward -188.18\n",
      "Starting episode:  83\n",
      "episode  83 total episode reward -104.65, avg episode reward -187.18\n",
      "Starting episode:  84\n",
      "episode  84 total episode reward -266.63, avg episode reward -188.12\n",
      "Starting episode:  85\n",
      "episode  85 total episode reward -79.76, avg episode reward -186.86\n",
      "Starting episode:  86\n",
      "episode  86 total episode reward -33.14, avg episode reward -185.09\n",
      "Starting episode:  87\n",
      "episode  87 total episode reward -360.55, avg episode reward -187.09\n",
      "Starting episode:  88\n",
      "episode  88 total episode reward -139.70, avg episode reward -186.55\n",
      "Starting episode:  89\n",
      "episode  89 total episode reward -117.07, avg episode reward -185.78\n",
      "Starting episode:  90\n",
      "episode  90 total episode reward -76.44, avg episode reward -184.58\n",
      "Starting episode:  91\n",
      "episode  91 total episode reward -318.72, avg episode reward -186.04\n",
      "Starting episode:  92\n",
      "episode  92 total episode reward -353.64, avg episode reward -187.84\n",
      "Starting episode:  93\n",
      "episode  93 total episode reward -92.08, avg episode reward -186.82\n",
      "Starting episode:  94\n",
      "episode  94 total episode reward -294.64, avg episode reward -187.96\n",
      "Starting episode:  95\n",
      "episode  95 total episode reward -471.79, avg episode reward -190.91\n",
      "Starting episode:  96\n",
      "episode  96 total episode reward -235.97, avg episode reward -191.38\n",
      "Starting episode:  97\n",
      "episode  97 total episode reward -192.86, avg episode reward -191.39\n",
      "Starting episode:  98\n",
      "episode  98 total episode reward -53.85, avg episode reward -190.00\n",
      "Starting episode:  99\n",
      "episode  99 total episode reward -221.08, avg episode reward -190.31\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "total_episode_reward_history = []\n",
    "avg_episode_reward_history = []\n",
    "num_episodes = 2000\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0\n",
    "\n",
    "    print('Starting episode: ', i)\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        actor_loss, critic_loss = agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_episode_reward += reward\n",
    "        \n",
    "    total_episode_reward_history.append(total_episode_reward)\n",
    "    avg_episode_reward = np.mean(total_episode_reward_history[-100:])\n",
    "    avg_episode_reward_history.append(avg_episode_reward)\n",
    "    \n",
    "    plot_history(total_episode_reward_history, avg_episode_reward_history,\n",
    "                 os.path.join(model_path, 'plot'))\n",
    "    print('episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "          (total_episode_reward, avg_episode_reward))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "np.savez(os.path.join(model_path, 'history'), score_history=score_history, avg_score_history=avg_score_history)\n",
    "\n",
    "# save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
