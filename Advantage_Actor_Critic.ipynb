{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU memory limitated successfuly!\n"
     ]
    }
   ],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Physical Devices: {}'.format(physical_devices))\n",
    "try:\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('GPU memory limitated successfuly!')\n",
    "except:\n",
    "    print('Warning! GPU memory could not be limitated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero poner el ambiente, mostrar el baseline, explicarlo y luego el algoritmo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an Environment and an Agent?<br>\n",
    "Take a look into the lunar Lander Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x7f30e81f1ac0>,\n",
       " 'action_space': Discrete(4),\n",
       " 'observation_space': Box(-inf, inf, (8,), float32),\n",
       " 'reward_range': (-inf, inf),\n",
       " 'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "  'video.frames_per_second': 50},\n",
       " '_max_episode_steps': 1000,\n",
       " '_elapsed_steps': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from libs.Utils import plot_history\n",
    "from libs.Utils import test_agent\n",
    "\n",
    "# Load environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "# Number of actions\n",
    "n_actions = env.action_space.n\n",
    "vars(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the LunarLander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  265.79505142923756\n",
      "Total Episode Reward:  273.82849267405766\n",
      "Total Episode Reward:  298.8136812676555\n",
      "Total Episode Reward:  288.49102776714193\n",
      "Total Episode Reward:  277.5043662396249\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, heuristic=True, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma=0.99, actor_lr=5e-6, critic_lr=5e-6):\n",
    "        self.gamma = gamma\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        probs = self.actor(np.array([state]))\n",
    "        # tfp.distributions returns a prob dist\n",
    "        # same as using np.random.choice([0,1,2,3], p=probs.numpy())\n",
    "        dist = tfp.distributions.Categorical(probs=probs.numpy(), dtype=tf.float32)\n",
    "        action = dist.sample().numpy()\n",
    "        \n",
    "        return int(action[0])\n",
    "    \n",
    "    # Note this is actually the performance measure J(theta)\n",
    "    def actor_loss(self, probs, action, advantage):\n",
    "        dist = tfp.distributions.Categorical(probs=probs, dtype=tf.float32)\n",
    "        log_probs = dist.log_prob(action)\n",
    "        # Since we are maximizing the agent's performance,  we need to add a minus -\n",
    "        # to actually maximize instead of minimize\n",
    "        loss = -log_probs * advantage\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # tf needs a bidimensional array as input:\n",
    "        state = np.array([state])\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # Set costum losses for the actor and the critic\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            action_probs = self.actor(state, training=True)\n",
    "            state_value = self.critic(state, training=True)\n",
    "            if done:\n",
    "                next_state_value = 0\n",
    "            else:\n",
    "                next_state_value = self.critic(next_state, training=True)\n",
    "            \n",
    "            advantage = reward + self.gamma * next_state_value - state_value\n",
    "            \n",
    "            # Agents Performance measure J(theta)\n",
    "            agent_loss = self.actor_loss(action_probs, action, advantage)\n",
    "            # Critic loss (MSE for one example) is basically an \n",
    "            # approximation of (v - v_hat)^2\n",
    "            critic_loss = advantage**2\n",
    "        \n",
    "        # Apply learning rule\n",
    "        actor_grads = actor_tape.gradient(agent_loss, \n",
    "                                          self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(critic_loss, \n",
    "                                            self.critic.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, \n",
    "                                                 self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, \n",
    "                                                  self.critic.trainable_variables))\n",
    "        \n",
    "        return agent_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Factor\n",
    "gamma = 0.99\n",
    "# Actor learning rate\n",
    "#actor_lr = 5e-6\n",
    "actor_lr = 1e-6\n",
    "# Critic learning rate\n",
    "critic_lr = 5e-6\n",
    "#critic_lr = 5e-5\n",
    "\n",
    "# Init agent\n",
    "agent = Agent(gamma=0.99, actor_lr=5e-6, critic_lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our agent performs without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -253.90722700399797\n",
      "Total Episode Reward:  -375.25820864155787\n",
      "Total Episode Reward:  -90.04640272231845\n",
      "Total Episode Reward:  -311.3934819786125\n",
      "Total Episode Reward:  -395.2617817137771\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project'\n",
    "os.makedirs(os.path.join(path, 'Models'), exist_ok=True)\n",
    "model_path = os.path.join(path, 'Models', 'Advantage_Actor_Critic')\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "best_model_path = os.path.join(model_path, 'Best_model')\n",
    "os.makedirs(best_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model so far...\n",
      "Episode  0 total episode reward -65.91, avg episode reward -65.91\n",
      "WARNING:tensorflow:From /home/hhughes/anaconda3/envs/dl_seminar/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  0 total episode reward -65.91, avg episode reward -65.91\n",
      "episode  100 total episode reward -432.43, avg episode reward -180.71\n",
      "episode  200 total episode reward -76.72, avg episode reward -191.46\n",
      "episode  300 total episode reward -175.84, avg episode reward -221.58\n",
      "episode  400 total episode reward -187.59, avg episode reward -189.52\n",
      "episode  500 total episode reward -15.84, avg episode reward -143.19\n",
      "episode  600 total episode reward -124.11, avg episode reward -134.27\n",
      "episode  700 total episode reward -97.08, avg episode reward -119.83\n",
      "episode  800 total episode reward 40.08, avg episode reward -77.10\n",
      "Saving best model so far...\n",
      "Episode  817 total episode reward -50.62, avg episode reward -65.66\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  820 total episode reward 212.86, avg episode reward -64.29\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  821 total episode reward 231.31, avg episode reward -62.91\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  827 total episode reward 213.29, avg episode reward -59.48\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  828 total episode reward -235.48, avg episode reward -58.37\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  900 total episode reward -56.17, avg episode reward -88.85\n",
      "Saving best model so far...\n",
      "Episode  974 total episode reward -93.06, avg episode reward -57.82\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  975 total episode reward -169.56, avg episode reward -57.55\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  976 total episode reward -197.84, avg episode reward -55.67\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  977 total episode reward -197.54, avg episode reward -54.49\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  978 total episode reward -30.05, avg episode reward -51.43\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  982 total episode reward 182.68, avg episode reward -46.41\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  983 total episode reward 199.58, avg episode reward -40.10\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  986 total episode reward -26.54, avg episode reward -38.17\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  987 total episode reward 284.85, avg episode reward -36.01\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  988 total episode reward -148.56, avg episode reward -35.58\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  989 total episode reward -160.57, avg episode reward -34.56\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  990 total episode reward 194.40, avg episode reward -28.98\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  994 total episode reward 235.95, avg episode reward -25.86\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  996 total episode reward -91.38, avg episode reward -23.51\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  998 total episode reward -60.90, avg episode reward -21.83\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1000 total episode reward -15.33, avg episode reward -26.55\n",
      "Saving best model so far...\n",
      "Episode  1008 total episode reward 184.24, avg episode reward -18.89\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1010 total episode reward 172.05, avg episode reward -17.45\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1024 total episode reward -2.67, avg episode reward -16.67\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1100 total episode reward 149.43, avg episode reward -72.90\n",
      "Saving best model so far...\n",
      "Episode  1181 total episode reward 11.19, avg episode reward -15.23\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1192 total episode reward 171.30, avg episode reward -14.58\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1193 total episode reward 152.54, avg episode reward -7.79\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1200 total episode reward -211.77, avg episode reward -28.08\n",
      "episode  1300 total episode reward 165.85, avg episode reward -13.10\n",
      "Saving best model so far...\n",
      "Episode  1351 total episode reward 185.61, avg episode reward -7.71\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1352 total episode reward 247.94, avg episode reward -7.28\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1381 total episode reward 168.08, avg episode reward -5.62\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1383 total episode reward -184.20, avg episode reward -4.39\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1384 total episode reward 198.18, avg episode reward 1.31\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1397 total episode reward 188.91, avg episode reward 1.91\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1400 total episode reward 27.74, avg episode reward 1.79\n",
      "Saving best model so far...\n",
      "Episode  1401 total episode reward 186.05, avg episode reward 2.12\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1408 total episode reward -73.75, avg episode reward 2.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1409 total episode reward -182.96, avg episode reward 3.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1500 total episode reward 180.06, avg episode reward -20.97\n",
      "episode  1600 total episode reward 182.80, avg episode reward -40.53\n",
      "Saving best model so far...\n",
      "Episode  1621 total episode reward 132.75, avg episode reward 6.92\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1622 total episode reward -61.59, avg episode reward 8.62\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1627 total episode reward 124.21, avg episode reward 9.28\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1628 total episode reward -96.47, avg episode reward 10.83\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1629 total episode reward 219.44, avg episode reward 11.43\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1630 total episode reward -58.65, avg episode reward 13.49\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1631 total episode reward 46.17, avg episode reward 17.41\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1632 total episode reward 166.47, avg episode reward 21.00\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1638 total episode reward 251.62, avg episode reward 21.13\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1700 total episode reward -278.85, avg episode reward -28.88\n",
      "episode  1800 total episode reward 70.92, avg episode reward 8.23\n",
      "Saving best model so far...\n",
      "Episode  1814 total episode reward 247.56, avg episode reward 23.31\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1900 total episode reward -203.08, avg episode reward -20.83\n",
      "Saving best model so far...\n",
      "Episode  1947 total episode reward 189.04, avg episode reward 24.18\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1948 total episode reward 75.88, avg episode reward 28.51\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1949 total episode reward -173.47, avg episode reward 29.48\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1950 total episode reward 188.81, avg episode reward 33.68\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1958 total episode reward 243.88, avg episode reward 35.17\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1959 total episode reward 160.17, avg episode reward 41.54\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1972 total episode reward -74.33, avg episode reward 42.04\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1976 total episode reward 241.80, avg episode reward 47.30\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1977 total episode reward -36.90, avg episode reward 47.44\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2000 total episode reward -196.85, avg episode reward 32.14\n",
      "episode  2100 total episode reward 54.93, avg episode reward 12.44\n",
      "Saving best model so far...\n",
      "Episode  2128 total episode reward 226.71, avg episode reward 47.81\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2200 total episode reward 22.47, avg episode reward -11.84\n",
      "Saving best model so far...\n",
      "Episode  2244 total episode reward 244.10, avg episode reward 51.91\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2245 total episode reward 227.19, avg episode reward 56.37\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2246 total episode reward -2.63, avg episode reward 59.04\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2247 total episode reward 194.22, avg episode reward 59.51\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2248 total episode reward 215.37, avg episode reward 64.09\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2250 total episode reward 156.89, avg episode reward 68.10\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2300 total episode reward -84.34, avg episode reward 46.84\n",
      "Saving best model so far...\n",
      "Episode  2350 total episode reward -38.06, avg episode reward 68.35\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2354 total episode reward 160.52, avg episode reward 68.59\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2400 total episode reward 207.12, avg episode reward 20.68\n",
      "episode  2500 total episode reward 284.97, avg episode reward 43.30\n",
      "Saving best model so far...\n",
      "Episode  2515 total episode reward 193.06, avg episode reward 69.95\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2516 total episode reward 110.98, avg episode reward 71.27\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2518 total episode reward 154.41, avg episode reward 74.09\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2520 total episode reward 211.07, avg episode reward 75.08\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2524 total episode reward 63.63, avg episode reward 75.39\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2526 total episode reward 220.82, avg episode reward 77.69\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2529 total episode reward 207.31, avg episode reward 78.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2530 total episode reward 225.71, avg episode reward 82.92\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2531 total episode reward 169.75, avg episode reward 83.66\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2532 total episode reward 199.23, avg episode reward 83.89\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2533 total episode reward 122.41, avg episode reward 85.32\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2534 total episode reward 216.86, avg episode reward 88.93\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2551 total episode reward 201.31, avg episode reward 91.29\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2552 total episode reward 178.95, avg episode reward 91.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2553 total episode reward -10.06, avg episode reward 92.18\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2554 total episode reward 215.10, avg episode reward 92.67\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2562 total episode reward -29.94, avg episode reward 95.29\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2600 total episode reward -13.07, avg episode reward 66.66\n",
      "episode  2700 total episode reward 232.83, avg episode reward 74.28\n",
      "episode  2800 total episode reward 249.54, avg episode reward 81.48\n",
      "Saving best model so far...\n",
      "Episode  2815 total episode reward 202.16, avg episode reward 95.47\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2816 total episode reward 89.90, avg episode reward 97.04\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2817 total episode reward -95.74, avg episode reward 98.62\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2818 total episode reward 269.94, avg episode reward 104.22\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2819 total episode reward 136.06, avg episode reward 106.14\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2820 total episode reward 200.16, avg episode reward 106.18\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2900 total episode reward -0.43, avg episode reward 44.17\n",
      "episode  3000 total episode reward 158.74, avg episode reward 73.39\n",
      "episode  3100 total episode reward 290.88, avg episode reward 68.44\n",
      "Saving best model so far...\n",
      "Episode  3136 total episode reward 184.53, avg episode reward 110.85\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3137 total episode reward 246.27, avg episode reward 111.18\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3141 total episode reward 184.19, avg episode reward 113.26\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3142 total episode reward 208.20, avg episode reward 116.78\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3143 total episode reward 232.65, avg episode reward 122.95\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3151 total episode reward 162.08, avg episode reward 123.08\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3160 total episode reward 234.85, avg episode reward 123.43\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3174 total episode reward 273.27, avg episode reward 124.68\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3175 total episode reward 26.55, avg episode reward 125.72\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3176 total episode reward 217.65, avg episode reward 131.54\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3200 total episode reward 152.78, avg episode reward 118.71\n",
      "episode  3300 total episode reward 241.96, avg episode reward 128.38\n",
      "Saving best model so far...\n",
      "Episode  3305 total episode reward 185.13, avg episode reward 133.02\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3307 total episode reward 44.82, avg episode reward 133.25\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3400 total episode reward 204.40, avg episode reward 124.19\n",
      "Saving best model so far...\n",
      "Episode  3449 total episode reward 117.48, avg episode reward 134.97\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3500 total episode reward 211.33, avg episode reward 94.38\n",
      "Saving best model so far...\n",
      "Episode  3551 total episode reward 225.95, avg episode reward 137.98\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3553 total episode reward 215.56, avg episode reward 140.14\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3557 total episode reward 195.45, avg episode reward 142.84\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3558 total episode reward 163.29, avg episode reward 145.07\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3559 total episode reward 211.33, avg episode reward 145.61\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3560 total episode reward 241.69, avg episode reward 151.02\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3562 total episode reward 195.52, avg episode reward 156.51\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3563 total episode reward 245.58, avg episode reward 157.08\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3564 total episode reward 223.09, avg episode reward 157.27\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3569 total episode reward 235.76, avg episode reward 158.26\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3600 total episode reward -5.48, avg episode reward 138.97\n",
      "episode  3700 total episode reward 238.08, avg episode reward 136.54\n",
      "episode  3800 total episode reward 1.81, avg episode reward 102.44\n",
      "episode  3900 total episode reward -99.74, avg episode reward -8.43\n",
      "episode  4000 total episode reward -155.44, avg episode reward -138.83\n",
      "episode  4100 total episode reward -159.32, avg episode reward -129.30\n",
      "episode  4200 total episode reward -127.61, avg episode reward -132.28\n",
      "episode  4300 total episode reward -146.51, avg episode reward -124.82\n",
      "episode  4400 total episode reward -160.74, avg episode reward -126.21\n",
      "episode  4500 total episode reward -109.33, avg episode reward -132.63\n",
      "episode  4600 total episode reward -83.60, avg episode reward -131.83\n",
      "episode  4700 total episode reward -113.21, avg episode reward -136.11\n",
      "episode  4800 total episode reward 25.64, avg episode reward -125.88\n",
      "episode  4900 total episode reward -201.20, avg episode reward -130.12\n",
      "episode  5000 total episode reward -141.09, avg episode reward -138.66\n",
      "episode  5100 total episode reward -229.62, avg episode reward -132.05\n",
      "episode  5199 total episode reward -131.01, avg episode reward -137.06\n",
      "Train time (in mins):  433.8906423846881\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Last_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "total_episode_reward_history = []\n",
    "avg_episode_reward_history = []\n",
    "best_avg_episode_reward = -9e6\n",
    "num_episodes = 5200\n",
    "total_n_inter = 0\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0\n",
    "\n",
    "    #print('Starting episode: ', i)\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        actor_loss, critic_loss = agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_episode_reward += reward\n",
    "        total_n_inter += total_n_inter + 1\n",
    "        \n",
    "    total_episode_reward_history.append(total_episode_reward)\n",
    "    avg_episode_reward = np.mean(total_episode_reward_history[-64:])\n",
    "    avg_episode_reward_history.append(avg_episode_reward)\n",
    "    \n",
    "    # Save best policy so far\n",
    "    if best_avg_episode_reward < avg_episode_reward:\n",
    "        print('Saving best model so far...')\n",
    "        print('Episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "              (total_episode_reward, avg_episode_reward))\n",
    "        best_avg_episode_reward = avg_episode_reward\n",
    "        agent.actor.save(best_model_path)\n",
    "        \n",
    "    \n",
    "    if (i % 100 == 0) | (i == (num_episodes-1)):\n",
    "        plot_history(total_episode_reward_history, avg_episode_reward_history,\n",
    "                     os.path.join(model_path, 'plot'))\n",
    "        print('episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "          (total_episode_reward, avg_episode_reward))\n",
    "print('Train time (in mins): ',  ((time.time()-tic) / 60))\n",
    "\n",
    "# Save history\n",
    "np.savez(os.path.join(model_path, 'history'), \n",
    "         total_episode_reward_history=total_episode_reward_history, \n",
    "         avg_episode_reward_history=avg_episode_reward_history)\n",
    "\n",
    "# Save last model\n",
    "last_model_path = os.path.join(model_path, 'Last_model')\n",
    "os.makedirs(last_model_path, exist_ok=True)\n",
    "agent.actor.save(last_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -136.3167456977772\n",
      "Total Episode Reward:  -154.437767639191\n",
      "Total Episode Reward:  -51.45886287316006\n",
      "Total Episode Reward:  -102.11646554520831\n",
      "Total Episode Reward:  -92.1034108990026\n"
     ]
    }
   ],
   "source": [
    "# Test last model\n",
    "#del(agent)\n",
    "#agent = Agent()\n",
    "#agent.actor = tf.keras.models.load_model(os.path.join(model_path, 'Last_model'))\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Total Episode Reward:  233.39374415338392\n",
      "Total Episode Reward:  13.873871520545634\n",
      "Total Episode Reward:  2.346754614134184\n",
      "Total Episode Reward:  148.43230568809872\n",
      "Total Episode Reward:  296.1235679190527\n"
     ]
    }
   ],
   "source": [
    "# Test best model\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "agent.actor = tf.keras.models.load_model(best_model_path)\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Total Episode Reward:  -167.72769755387748\n",
      "Total Episode Reward:  -30.884272412830853\n",
      "Total Episode Reward:  47.111053623166754\n",
      "Total Episode Reward:  201.73836078023513\n",
      "Total Episode Reward:  -184.62839979410555\n"
     ]
    }
   ],
   "source": [
    "# Test intermediant model\n",
    "inter_model_path = os.path.join(model_path, 'Intermediant_model')\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "agent.actor = tf.keras.models.load_model(inter_model_path)\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -74.6144368701255\n",
      "Total Episode Reward:  -234.27694124610758\n",
      "Total Episode Reward:  -186.26175254617286\n",
      "Total Episode Reward:  -297.392797095341\n",
      "Total Episode Reward:  -215.51888556515956\n"
     ]
    }
   ],
   "source": [
    "# Test model without training\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
