Policy Gradient methods
-------------------------------------------------------------------------------
- To ensure exploration we generally require that the policy never becomes deterministic (i.e., that π(a|s, θ) in (0, 1),
for all s, a, θ).
-Policy-based methods also offer useful ways of dealing with continuous action spaces.
-Perhaps the simplest advantage that policy parameterization may have over action-value parameterization is that the policy may be a simpler function to approximate.

Baseline:
- A way to reduce variance is to have a baseline

Trues region:
-be careful with updates (because you are changing the policy). Recall that RL learns from data generated by the environment and the inteactions with it (i.e. the policy). Therefore, a strong change in the policy means a strong change in the data distribution. If the policy is bad, then the data will be bad and it will be very dificult to learn something meaningful.
- This is different from the supervsed learning case, where the learning and data are tipically independent. There you just have a dataset and you cant mess it up during your training process, wile in RL you can.
-Therefore, to prevent instability, you can limit the difference between subsequent policies

-KL divergence is like a distance metric but is defined between distributions and is not actually symmetric, so therefore is not a metric. You can think about it as the l1 or l2 regularization used in regression for example.

- an alternative to this is to use large batches to reduce the variance.

- In "Trust region policy optimization" (TRPO) the regularizer defines a region in which we trust our new policy 
