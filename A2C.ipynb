{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple implementation of a Deep Reinforcement Learning Advantage Actor-Critic. It uses 2 independent Artificial Neural Networks to approximate the Policy function (Actor) and the State-value function (Critic). To test the implementation, I use the Moon Lander environment provided by OpenAI-Gym.\n",
    "\n",
    "If you want to have a deeper understanding of the Actor-Critic algorithm, I strongly recomend you to take a look into the document `References/A2C_Summary/A2C_Summary.pdf` and `References/A2C_Presentation.pdf`. In the directory `References/A2C_Summary/` you can also find the original $\\LaTeX$ document used to create the symmary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices: []\n",
      "GPU memory limitated successfuly!\n"
     ]
    }
   ],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from libs.Utils import plot_history\n",
    "from libs.Utils import test_agent\n",
    "\n",
    "# Uncomment the next line if you want tf to ignore your GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Physical Devices: {}'.format(physical_devices))\n",
    "try:\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('GPU memory limitated successfuly!')\n",
    "except:\n",
    "    print('Warning! GPU memory could not be limitated!')\n",
    "    \n",
    "# Define the places to save the models\n",
    "path = './'\n",
    "model_path = os.path.join(path, 'Models')\n",
    "# Create dirs to save models\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "best_model_path = os.path.join(model_path, 'Best_model')\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "last_model_path = os.path.join(model_path, 'Last_model')\n",
    "os.makedirs(last_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Load Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x7f0e24c5b610>,\n",
       " 'action_space': Discrete(4),\n",
       " 'observation_space': Box(-inf, inf, (8,), float32),\n",
       " 'reward_range': (-inf, inf),\n",
       " 'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "  'video.frames_per_second': 50},\n",
       " '_max_episode_steps': 1000,\n",
       " '_elapsed_steps': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "# Number of actions\n",
    "n_actions = env.action_space.n\n",
    "vars(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the LunarLander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  265.3090217637236\n",
      "Total Episode Reward:  287.0427874891634\n",
      "Total Episode Reward:  257.1252867981133\n",
      "Total Episode Reward:  299.9156735822128\n",
      "Total Episode Reward:  273.23138272232137\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, heuristic=True, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.- The Actor and the Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma=0.99, actor_lr=5e-6, critic_lr=5e-6):\n",
    "        self.gamma = gamma\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        probs = self.actor(np.array([state]))\n",
    "        # tfp.distributions returns a prob dist\n",
    "        # same as using np.random.choice([0,1,2,3], p=probs.numpy())\n",
    "        dist = tfp.distributions.Categorical(probs=probs.numpy(), dtype=tf.float32)\n",
    "        action = dist.sample().numpy()\n",
    "        \n",
    "        return int(action[0])\n",
    "    \n",
    "    # Note this is actually the performance measure J(theta)\n",
    "    def actor_loss(self, probs, action, advantage):\n",
    "        dist = tfp.distributions.Categorical(probs=probs, dtype=tf.float32)\n",
    "        log_probs = dist.log_prob(action)\n",
    "        # Since we are maximizing the agent's performance,  we need to add a minus -\n",
    "        # to actually maximize instead of minimize\n",
    "        loss = -log_probs * advantage\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # tf needs a bidimensional array as input:\n",
    "        state = np.array([state])\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # Set costum losses for the actor and the critic\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            action_probs = self.actor(state, training=True)\n",
    "            state_value = self.critic(state, training=True)\n",
    "            if done:\n",
    "                next_state_value = 0\n",
    "            else:\n",
    "                next_state_value = self.critic(next_state, training=True)\n",
    "            \n",
    "            advantage = reward + self.gamma * next_state_value - state_value\n",
    "            \n",
    "            # Agents Performance measure J(theta)\n",
    "            agent_loss = self.actor_loss(action_probs, action, advantage)\n",
    "            # Critic loss (MSE for one example) is basically an \n",
    "            # approximation of (v - v_hat)^2\n",
    "            critic_loss = advantage**2\n",
    "        \n",
    "        # Apply learning rule\n",
    "        actor_grads = actor_tape.gradient(agent_loss, \n",
    "                                          self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(critic_loss, \n",
    "                                            self.critic.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, \n",
    "                                                 self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, \n",
    "                                                  self.critic.trainable_variables))\n",
    "        \n",
    "        return agent_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.- Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Factor\n",
    "gamma = 0.99\n",
    "# Actor learning rate\n",
    "actor_lr = 1e-6\n",
    "# Critic learning rate\n",
    "critic_lr = 5e-6\n",
    "\n",
    "# Init agent\n",
    "agent = Agent(gamma=0.99, actor_lr=5e-6, critic_lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our agent performs without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -89.99154013467287\n",
      "Total Episode Reward:  -229.3687217007442\n",
      "Total Episode Reward:  -99.52781041097954\n",
      "Total Episode Reward:  -123.32629389526207\n",
      "Total Episode Reward:  -182.571081715751\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.- Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model so far...\n",
      "Episode  0 total episode reward -301.39, avg episode reward -301.39\n",
      "WARNING:tensorflow:From /home/hhughes/anaconda3/envs/dl_seminar/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  0 total episode reward -301.39, avg episode reward -301.39\n",
      "Saving best model so far...\n",
      "Episode  1 total episode reward -91.36, avg episode reward -196.37\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2 total episode reward -93.06, avg episode reward -161.93\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  6 total episode reward -76.39, avg episode reward -156.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  8 total episode reward -88.72, avg episode reward -149.57\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  9 total episode reward -119.42, avg episode reward -146.56\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  100 total episode reward -201.44, avg episode reward -182.09\n",
      "episode  200 total episode reward -188.44, avg episode reward -192.24\n",
      "episode  300 total episode reward -387.48, avg episode reward -202.01\n",
      "episode  400 total episode reward -265.48, avg episode reward -164.23\n",
      "episode  500 total episode reward -258.66, avg episode reward -159.29\n",
      "Saving best model so far...\n",
      "Episode  539 total episode reward -54.59, avg episode reward -146.47\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  541 total episode reward 35.03, avg episode reward -145.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  542 total episode reward -208.20, avg episode reward -145.36\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  543 total episode reward -41.86, avg episode reward -144.18\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  562 total episode reward -82.27, avg episode reward -140.99\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  565 total episode reward -20.39, avg episode reward -137.90\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  574 total episode reward -26.19, avg episode reward -137.36\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  575 total episode reward -35.20, avg episode reward -137.19\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  579 total episode reward 129.19, avg episode reward -133.78\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  583 total episode reward 86.07, avg episode reward -132.68\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  584 total episode reward -39.86, avg episode reward -131.30\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  600 total episode reward -250.07, avg episode reward -141.88\n",
      "episode  700 total episode reward -77.93, avg episode reward -148.73\n",
      "Saving best model so far...\n",
      "Episode  755 total episode reward -81.87, avg episode reward -129.55\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  757 total episode reward -1.57, avg episode reward -127.81\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  758 total episode reward -86.95, avg episode reward -125.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  776 total episode reward -36.92, avg episode reward -123.44\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  777 total episode reward 13.52, avg episode reward -118.62\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  786 total episode reward -170.68, avg episode reward -118.48\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  789 total episode reward -64.10, avg episode reward -117.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  790 total episode reward -54.25, avg episode reward -115.68\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  791 total episode reward -222.31, avg episode reward -115.60\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  794 total episode reward 112.42, avg episode reward -115.28\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  795 total episode reward -186.68, avg episode reward -113.83\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  796 total episode reward -60.71, avg episode reward -110.43\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  797 total episode reward -19.38, avg episode reward -110.40\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  798 total episode reward -64.04, avg episode reward -107.87\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  799 total episode reward -182.78, avg episode reward -107.54\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  800 total episode reward 114.47, avg episode reward -102.81\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  800 total episode reward 114.47, avg episode reward -102.81\n",
      "Saving best model so far...\n",
      "Episode  801 total episode reward 4.14, avg episode reward -102.19\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  802 total episode reward -24.81, avg episode reward -99.44\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  808 total episode reward -157.55, avg episode reward -99.02\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  809 total episode reward -11.67, avg episode reward -95.66\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  810 total episode reward -186.80, avg episode reward -94.59\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  813 total episode reward 62.22, avg episode reward -94.25\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  814 total episode reward -8.39, avg episode reward -91.50\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  822 total episode reward 91.66, avg episode reward -88.98\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  826 total episode reward 94.89, avg episode reward -86.37\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  829 total episode reward -13.85, avg episode reward -84.71\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  830 total episode reward -60.05, avg episode reward -82.99\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  831 total episode reward -191.17, avg episode reward -81.83\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  832 total episode reward -253.30, avg episode reward -81.79\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  833 total episode reward 19.69, avg episode reward -78.41\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  834 total episode reward 60.85, avg episode reward -76.89\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  839 total episode reward 68.51, avg episode reward -76.54\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  842 total episode reward 72.84, avg episode reward -75.17\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  843 total episode reward 97.15, avg episode reward -70.39\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  850 total episode reward 171.31, avg episode reward -65.59\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  852 total episode reward 130.62, avg episode reward -63.31\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  857 total episode reward 7.81, avg episode reward -61.99\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  900 total episode reward 185.52, avg episode reward -78.11\n",
      "Saving best model so far...\n",
      "Episode  912 total episode reward -47.71, avg episode reward -61.14\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  915 total episode reward 159.38, avg episode reward -61.11\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  916 total episode reward 162.80, avg episode reward -60.60\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  917 total episode reward -63.76, avg episode reward -59.38\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  918 total episode reward 239.26, avg episode reward -52.91\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  919 total episode reward 154.72, avg episode reward -47.04\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  924 total episode reward 220.08, avg episode reward -44.65\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  925 total episode reward 240.53, avg episode reward -36.87\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  926 total episode reward 181.34, avg episode reward -33.01\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  931 total episode reward -1.48, avg episode reward -31.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  939 total episode reward 164.41, avg episode reward -31.34\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  940 total episode reward -200.70, avg episode reward -30.44\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  941 total episode reward -162.15, avg episode reward -30.28\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  942 total episode reward -32.30, avg episode reward -29.58\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  943 total episode reward 150.82, avg episode reward -25.48\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  944 total episode reward -86.52, avg episode reward -24.37\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  945 total episode reward -13.94, avg episode reward -20.91\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  947 total episode reward 267.15, avg episode reward -18.40\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  950 total episode reward 206.22, avg episode reward -16.29\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  951 total episode reward 13.30, avg episode reward -15.33\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  959 total episode reward 231.33, avg episode reward -14.04\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  960 total episode reward -11.17, avg episode reward -10.43\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1000 total episode reward -191.63, avg episode reward -31.42\n",
      "episode  1100 total episode reward -40.15, avg episode reward -66.64\n",
      "episode  1200 total episode reward -227.34, avg episode reward -33.93\n",
      "Saving best model so far...\n",
      "Episode  1210 total episode reward 33.62, avg episode reward -10.03\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1212 total episode reward 178.09, avg episode reward -7.60\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1213 total episode reward 167.23, avg episode reward -1.65\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1216 total episode reward -95.83, avg episode reward -0.49\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1300 total episode reward 228.31, avg episode reward -57.49\n",
      "episode  1400 total episode reward 175.88, avg episode reward -46.96\n",
      "episode  1500 total episode reward -261.89, avg episode reward -55.94\n",
      "Saving best model so far...\n",
      "Episode  1589 total episode reward 142.88, avg episode reward 3.05\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1591 total episode reward -20.45, avg episode reward 5.22\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1600 total episode reward -37.81, avg episode reward -14.86\n",
      "Saving best model so far...\n",
      "Episode  1624 total episode reward 32.97, avg episode reward 5.38\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1626 total episode reward 177.25, avg episode reward 7.86\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1630 total episode reward 170.68, avg episode reward 8.49\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1631 total episode reward 192.36, avg episode reward 12.32\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1632 total episode reward 153.96, avg episode reward 15.49\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1698 total episode reward 259.23, avg episode reward 15.73\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1700 total episode reward 251.24, avg episode reward 18.17\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1700 total episode reward 251.24, avg episode reward 18.17\n",
      "Saving best model so far...\n",
      "Episode  1702 total episode reward 182.65, avg episode reward 22.71\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1704 total episode reward -155.92, avg episode reward 23.50\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1705 total episode reward -198.39, avg episode reward 23.58\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1706 total episode reward -47.79, avg episode reward 26.27\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1707 total episode reward -41.96, avg episode reward 28.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1708 total episode reward -4.51, avg episode reward 33.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1724 total episode reward 190.55, avg episode reward 33.43\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1736 total episode reward 229.17, avg episode reward 34.94\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1800 total episode reward -168.56, avg episode reward -47.81\n",
      "Saving best model so far...\n",
      "Episode  1889 total episode reward 196.11, avg episode reward 36.52\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1890 total episode reward -28.04, avg episode reward 39.41\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1897 total episode reward -32.17, avg episode reward 41.35\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1898 total episode reward 249.14, avg episode reward 48.58\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1899 total episode reward 119.19, avg episode reward 53.76\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  1900 total episode reward 196.97, avg episode reward 53.38\n",
      "Saving best model so far...\n",
      "Episode  1912 total episode reward 176.34, avg episode reward 56.13\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1913 total episode reward -176.14, avg episode reward 56.68\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1914 total episode reward 187.67, avg episode reward 57.08\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1915 total episode reward -185.92, avg episode reward 58.49\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1922 total episode reward 215.28, avg episode reward 62.47\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1923 total episode reward 189.83, avg episode reward 65.71\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1947 total episode reward 113.50, avg episode reward 67.36\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1960 total episode reward 228.18, avg episode reward 68.17\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1961 total episode reward 235.74, avg episode reward 72.36\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1967 total episode reward 242.23, avg episode reward 75.12\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1968 total episode reward -21.82, avg episode reward 78.24\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  1980 total episode reward 217.93, avg episode reward 80.59\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2000 total episode reward 149.13, avg episode reward 58.33\n",
      "episode  2100 total episode reward -195.26, avg episode reward 28.44\n",
      "episode  2200 total episode reward -223.70, avg episode reward 17.19\n",
      "episode  2300 total episode reward 203.53, avg episode reward 56.54\n",
      "Saving best model so far...\n",
      "Episode  2326 total episode reward 218.30, avg episode reward 81.18\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2327 total episode reward -47.28, avg episode reward 83.22\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2328 total episode reward 243.07, avg episode reward 89.79\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2329 total episode reward 210.22, avg episode reward 89.95\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2330 total episode reward -7.25, avg episode reward 93.66\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2343 total episode reward 183.60, avg episode reward 95.77\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2400 total episode reward 188.14, avg episode reward 39.60\n",
      "episode  2500 total episode reward -211.13, avg episode reward 58.89\n",
      "Saving best model so far...\n",
      "Episode  2564 total episode reward -161.73, avg episode reward 95.98\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2567 total episode reward 206.50, avg episode reward 98.96\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2600 total episode reward -170.23, avg episode reward 86.79\n",
      "Saving best model so far...\n",
      "Episode  2629 total episode reward 237.74, avg episode reward 103.06\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2635 total episode reward 132.24, avg episode reward 103.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2636 total episode reward 219.77, avg episode reward 109.83\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2637 total episode reward 242.26, avg episode reward 111.91\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  2638 total episode reward 193.54, avg episode reward 117.69\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  2700 total episode reward 210.59, avg episode reward 47.87\n",
      "episode  2800 total episode reward 17.40, avg episode reward 71.46\n",
      "episode  2900 total episode reward 232.31, avg episode reward 77.67\n",
      "episode  3000 total episode reward 3.85, avg episode reward 93.11\n",
      "episode  3100 total episode reward 196.07, avg episode reward 90.71\n",
      "episode  3200 total episode reward 207.74, avg episode reward 88.67\n",
      "episode  3300 total episode reward -209.87, avg episode reward 92.56\n",
      "Saving best model so far...\n",
      "Episode  3333 total episode reward 180.37, avg episode reward 119.00\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3335 total episode reward 279.38, avg episode reward 119.77\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3341 total episode reward 202.38, avg episode reward 119.81\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3376 total episode reward 232.86, avg episode reward 122.31\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3383 total episode reward 243.37, avg episode reward 126.26\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3384 total episode reward 32.64, avg episode reward 126.71\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3387 total episode reward 255.29, avg episode reward 130.68\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3388 total episode reward 246.03, avg episode reward 137.25\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3389 total episode reward 226.81, avg episode reward 137.67\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3400 total episode reward 251.10, avg episode reward 129.61\n",
      "episode  3500 total episode reward 209.81, avg episode reward 101.71\n",
      "Saving best model so far...\n",
      "Episode  3595 total episode reward 245.43, avg episode reward 139.36\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3600 total episode reward -125.96, avg episode reward 125.63\n",
      "episode  3700 total episode reward 198.12, avg episode reward 119.40\n",
      "Saving best model so far...\n",
      "Episode  3744 total episode reward 278.29, avg episode reward 139.73\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3745 total episode reward 248.42, avg episode reward 140.14\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3746 total episode reward 231.73, avg episode reward 140.63\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3748 total episode reward 137.89, avg episode reward 142.25\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3749 total episode reward 299.18, avg episode reward 143.25\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3750 total episode reward 11.92, avg episode reward 143.29\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3752 total episode reward 265.15, avg episode reward 148.09\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3773 total episode reward 289.11, avg episode reward 150.56\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3774 total episode reward 270.14, avg episode reward 150.80\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3776 total episode reward 290.55, avg episode reward 155.32\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3777 total episode reward 234.79, avg episode reward 155.99\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  3800 total episode reward -10.39, avg episode reward 124.21\n",
      "episode  3900 total episode reward 235.38, avg episode reward 139.22\n",
      "Saving best model so far...\n",
      "Episode  3939 total episode reward 252.17, avg episode reward 156.39\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3940 total episode reward 227.02, avg episode reward 159.28\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3946 total episode reward 216.30, avg episode reward 159.32\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  3947 total episode reward 41.53, avg episode reward 160.23\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  4000 total episode reward -20.51, avg episode reward 128.80\n",
      "episode  4100 total episode reward -51.60, avg episode reward 113.54\n",
      "episode  4200 total episode reward 9.76, avg episode reward 134.39\n",
      "episode  4300 total episode reward 8.86, avg episode reward 113.06\n",
      "episode  4400 total episode reward 4.08, avg episode reward 124.74\n",
      "Saving best model so far...\n",
      "Episode  4491 total episode reward 248.72, avg episode reward 161.87\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  4500 total episode reward 112.75, avg episode reward 146.16\n",
      "episode  4600 total episode reward 7.91, avg episode reward 118.62\n",
      "Saving best model so far...\n",
      "Episode  4627 total episode reward 191.19, avg episode reward 164.20\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  4628 total episode reward 219.13, avg episode reward 166.15\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  4700 total episode reward 265.42, avg episode reward 125.69\n",
      "episode  4800 total episode reward 135.90, avg episode reward 159.89\n",
      "Saving best model so far...\n",
      "Episode  4816 total episode reward 202.44, avg episode reward 168.45\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  4817 total episode reward 249.57, avg episode reward 168.78\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  4818 total episode reward 266.89, avg episode reward 174.83\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "Saving best model so far...\n",
      "Episode  4823 total episode reward 254.53, avg episode reward 175.55\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Best_model/assets\n",
      "episode  4900 total episode reward -74.72, avg episode reward 136.78\n",
      "episode  5000 total episode reward -105.35, avg episode reward -134.64\n",
      "episode  5100 total episode reward -131.96, avg episode reward -128.28\n",
      "episode  5199 total episode reward -100.24, avg episode reward -131.32\n",
      "Train time (in mins):  505.58613684177396\n",
      "INFO:tensorflow:Assets written to: /home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/Models/Advantage_Actor_Critic/Last_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "total_episode_reward_history = []\n",
    "avg_episode_reward_history = []\n",
    "best_avg_episode_reward = -9e6\n",
    "num_episodes = 5200\n",
    "total_n_inter = 0\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0\n",
    "\n",
    "    #print('Starting episode: ', i)\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        actor_loss, critic_loss = agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_episode_reward += reward\n",
    "        total_n_inter += total_n_inter + 1\n",
    "        \n",
    "    total_episode_reward_history.append(total_episode_reward)\n",
    "    avg_episode_reward = np.mean(total_episode_reward_history[-64:])\n",
    "    avg_episode_reward_history.append(avg_episode_reward)\n",
    "    \n",
    "    # Save best policy so far\n",
    "    if best_avg_episode_reward < avg_episode_reward:\n",
    "        print('Saving best model so far...')\n",
    "        print('Episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "              (total_episode_reward, avg_episode_reward))\n",
    "        best_avg_episode_reward = avg_episode_reward\n",
    "        agent.actor.save(best_model_path)\n",
    "        \n",
    "    \n",
    "    if (i % 100 == 0) | (i == (num_episodes-1)):\n",
    "        plot_history(total_episode_reward_history, avg_episode_reward_history,\n",
    "                     os.path.join(model_path, 'plot'))\n",
    "        print('episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "          (total_episode_reward, avg_episode_reward))\n",
    "print('Train time (in mins): ',  ((time.time()-tic) / 60))\n",
    "# print(total_n_inter)\n",
    "\n",
    "# Save history\n",
    "np.savez(os.path.join(model_path, 'history'), \n",
    "         total_episode_reward_history=total_episode_reward_history, \n",
    "         avg_episode_reward_history=avg_episode_reward_history)\n",
    "\n",
    "# Save last model\n",
    "agent.actor.save(last_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Test the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.- Test last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -157.83038452377954\n",
      "Total Episode Reward:  -99.72781729128329\n",
      "Total Episode Reward:  -135.60894452858483\n",
      "Total Episode Reward:  -130.16759846478797\n",
      "Total Episode Reward:  -156.41825497118842\n"
     ]
    }
   ],
   "source": [
    "# Uncomment next lines if you want to reload the last model\n",
    "#del(agent)\n",
    "#agent = Agent()\n",
    "#agent.actor = tf.keras.models.load_model(last_model_path)\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.- Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Total Episode Reward:  274.0378887860743\n",
      "Total Episode Reward:  221.85726019993933\n",
      "Total Episode Reward:  -14.467176654759323\n",
      "Total Episode Reward:  250.0321871721154\n",
      "Total Episode Reward:  1.122796094880158\n"
     ]
    }
   ],
   "source": [
    "# Test best model\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "agent.actor = tf.keras.models.load_model(best_model_path)\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.-References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
