{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices: []\n",
      "GPU memory limitated successfuly!\n"
     ]
    }
   ],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Physical Devices: {}'.format(physical_devices))\n",
    "try:\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('GPU memory limitated successfuly!')\n",
    "except:\n",
    "    print('Warning! GPU memory could not be limitated!')\n",
    "    \n",
    "path = '/home/hhughes/Documents/TUM_Subjects/S5/Seminar/Seminar_Project'\n",
    "os.makedirs(os.path.join(path, 'Models'), exist_ok=True)\n",
    "model_path = os.path.join(path, 'Models', 'For_presentation')\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "best_model_path = os.path.join(model_path, 'Best_model')\n",
    "os.makedirs(best_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x7fb2802094f0>,\n",
       " 'action_space': Discrete(4),\n",
       " 'observation_space': Box(-inf, inf, (8,), float32),\n",
       " 'reward_range': (-inf, inf),\n",
       " 'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "  'video.frames_per_second': 50},\n",
       " '_max_episode_steps': 1000,\n",
       " '_elapsed_steps': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from libs.Utils import plot_history\n",
    "from libs.Utils import test_agent\n",
    "\n",
    "# Load environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "# Number of actions\n",
    "n_actions = env.action_space.n\n",
    "vars(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video('Moon_Lander.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v2 Environment description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Landing pad is always at coordinates (0,0). \n",
    "- Coordinates are the first two numbers in state vector.\n",
    "- Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points.\n",
    "- Landing outside landing pad is possible. \n",
    "- Fuel is infinite, so an agent can learn to fly and then land on its first attempt. \n",
    "- Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the LunarLander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  272.21685030411015\n",
      "Total Episode Reward:  251.08950599307943\n",
      "Total Episode Reward:  257.95846565069485\n",
      "Total Episode Reward:  194.3517504190541\n",
      "Total Episode Reward:  23.156433943354656\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, heuristic=True, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, input_data):\n",
    "        x = self.dense1(input_data)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma=0.99, actor_lr=5e-6, critic_lr=5e-6):\n",
    "        self.gamma = gamma\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        probs = self.actor(np.array([state]))\n",
    "        # tfp.distributions returns a prob dist\n",
    "        # same as using np.random.choice([0,1,2,3], p=probs.numpy())\n",
    "        dist = tfp.distributions.Categorical(probs=probs.numpy(), dtype=tf.float32)\n",
    "        action = dist.sample().numpy()\n",
    "        \n",
    "        return int(action[0])\n",
    "    \n",
    "    # Note this is actually the performance measure J(theta)\n",
    "    def actor_loss(self, probs, action, advantage):\n",
    "        dist = tfp.distributions.Categorical(probs=probs, dtype=tf.float32)\n",
    "        log_probs = dist.log_prob(action)\n",
    "        # Since we are maximizing the agent's performance,  we need to add a minus -\n",
    "        # to actually maximize instead of minimize\n",
    "        loss = -log_probs * advantage\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # tf needs a bidimensional array as input:\n",
    "        state = np.array([state])\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        # Set costum losses for the actor and the critic\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            action_probs = self.actor(state, training=True)\n",
    "            state_value = self.critic(state, training=True)\n",
    "            if done:\n",
    "                next_state_value = 0\n",
    "            else:\n",
    "                next_state_value = self.critic(next_state, training=True)\n",
    "            \n",
    "            advantage = reward + self.gamma * next_state_value - state_value\n",
    "            \n",
    "            # Agents Performance measure J(theta)\n",
    "            agent_loss = self.actor_loss(action_probs, action, advantage)\n",
    "            # Critic loss (MSE for one example) is basically an \n",
    "            # approximation of (v - v_hat)^2\n",
    "            critic_loss = advantage**2\n",
    "        \n",
    "        # Apply learning rule\n",
    "        actor_grads = actor_tape.gradient(agent_loss, \n",
    "                                          self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(critic_loss, \n",
    "                                            self.critic.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, \n",
    "                                                 self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, \n",
    "                                                  self.critic.trainable_variables))\n",
    "        \n",
    "        return agent_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Factor\n",
    "gamma = 0.99\n",
    "# Actor learning rate\n",
    "#actor_lr = 5e-6\n",
    "actor_lr = 1e-6\n",
    "# Critic learning rate\n",
    "critic_lr = 5e-6\n",
    "#critic_lr = 5e-5\n",
    "\n",
    "# Init agent\n",
    "agent = Agent(gamma=0.99, actor_lr=5e-6, critic_lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our agent performs without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episode Reward:  -135.17463301430473\n",
      "Total Episode Reward:  -296.78184664710193\n",
      "Total Episode Reward:  -146.20941745252622\n",
      "Total Episode Reward:  -132.83840163979653\n",
      "Total Episode Reward:  -209.81610378393535\n",
      "Total Episode Reward:  -94.2150388613217\n",
      "Total Episode Reward:  -65.00133809390107\n",
      "Total Episode Reward:  -122.33731847804714\n",
      "Total Episode Reward:  -348.6521250885303\n",
      "Total Episode Reward:  -348.5806769148139\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, agent, render=True, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```python\n",
    "# Training loop\n",
    "total_episode_reward_history = []\n",
    "avg_episode_reward_history = []\n",
    "best_avg_episode_reward = -9e6\n",
    "num_episodes = 10000\n",
    "total_n_inter = 0\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0\n",
    "\n",
    "    #print('Starting episode: ', i)\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        actor_loss, critic_loss = agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_episode_reward += reward\n",
    "        total_n_inter += total_n_inter + 1\n",
    "        \n",
    "    total_episode_reward_history.append(total_episode_reward)\n",
    "    avg_episode_reward = np.mean(total_episode_reward_history[-64:])\n",
    "    avg_episode_reward_history.append(avg_episode_reward)\n",
    "    \n",
    "    # Save best policy so far\n",
    "    if best_avg_episode_reward < avg_episode_reward:\n",
    "        print('Saving best model so far...')\n",
    "        print('Episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "              (total_episode_reward, avg_episode_reward))\n",
    "        best_avg_episode_reward = avg_episode_reward\n",
    "        agent.actor.save(best_model_path)\n",
    "        \n",
    "    \n",
    "    if (i % 100 == 0) | (i == (num_episodes-1)):\n",
    "        plot_history(total_episode_reward_history, avg_episode_reward_history,\n",
    "                     os.path.join(model_path, 'plot'))\n",
    "        print('episode ', i, 'total episode reward %.2f, avg episode reward %.2f' % \\\n",
    "          (total_episode_reward, avg_episode_reward))\n",
    "print('Train time (in mins): ',  ((time.time()-tic) / 60))\n",
    "\n",
    "# Save last model\n",
    "last_model_path = os.path.join(model_path, 'Last_model')\n",
    "os.makedirs(last_model_path, exist_ok=True)\n",
    "agent.actor.save(last_model_path)\n",
    "\n",
    "# Save history\n",
    "np.savez(os.path.join(model_path, 'history'), \n",
    "         total_episode_reward_history=total_episode_reward_history, \n",
    "         avg_episode_reward_history=avg_episode_reward_history)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training took about 8 houres and more than 5000 Episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Total Episode Reward:  255.08862152046314\n",
      "Total Episode Reward:  -86.13646123476502\n",
      "Total Episode Reward:  261.39396068880734\n",
      "Total Episode Reward:  65.91627534398958\n",
      "Total Episode Reward:  250.30124504959284\n"
     ]
    }
   ],
   "source": [
    "# Test best model\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "agent.actor = tf.keras.models.load_model(best_model_path)\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Total Episode Reward:  262.97689049859935\n",
      "Total Episode Reward:  211.47017145061747\n",
      "Total Episode Reward:  263.0573789450133\n",
      "Total Episode Reward:  283.58904973223537\n",
      "Total Episode Reward:  257.7805683820527\n"
     ]
    }
   ],
   "source": [
    "# Test intermediant model\n",
    "inter_model_path = os.path.join(model_path, 'Intermediant_model')\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "agent.actor = tf.keras.models.load_model(inter_model_path)\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what happend at the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./Models/For_presentation/plot.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "4 (<class 'int'>) invalid ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b00e9a3d3bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Last_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/TUM_Subjects/S5/Seminar/Seminar_Project/libs/Utils.py\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(env, agent, heuristic, seed, render, n_episodes)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl_seminar/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl_seminar/lib/python3.8/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Engines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 4 (<class 'int'>) invalid "
     ]
    }
   ],
   "source": [
    "# Test last model\n",
    "del(agent)\n",
    "agent = Agent()\n",
    "agent.actor = tf.keras.models.load_model(os.path.join(model_path, 'Last_model'))\n",
    "test_agent(env, agent, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
